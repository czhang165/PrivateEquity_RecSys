{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Model Evaluation\n", "Comprehensive evaluation of Two-Tower and Deep Ranking models"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import torch\n", "import json\n", "import pickle\n", "from pathlib import Path\n", "import matplotlib.pyplot as plt\n", "\n", "# Import evaluation functions\n", "from evaluation import (\n", "    load_trained_models,\n", "    evaluate_metrics_implicit,\n", "    evaluate_hit_ratio,\n", "    evaluate_two_stage_pipeline,\n", "    plot_score_distributions\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Load Models and Data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load training summary\n", "with open('models/training_summary.json', 'r') as f:\n", "    summary = json.load(f)\n", "\n", "# Load feature encoder\n", "with open('models/feature_encoder.pkl', 'rb') as f:\n", "    encoder = pickle.load(f)\n", "\n", "# Load data\n", "interactions_df = pd.read_csv('data/enhanced_interactions.csv', parse_dates=['timestamp'])\n", "investor_df = pd.read_csv('data/investor_features.csv')\n", "deal_df = pd.read_csv('data/deal_features.csv')\n", "\n", "# Split data (same as training)\n", "interactions_df = interactions_df.sort_values('timestamp')\n", "train_interactions = interactions_df.iloc[:-100]\n", "test_interactions = interactions_df.iloc[-100:]\n", "\n", "print(f\"Test set: {len(test_interactions)} interactions\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load trained models\n", "two_tower_model, deep_ranking_model = load_trained_models(\n", "    two_tower_checkpoint_path=summary['two_tower_best_ckpt'],\n", "    deep_ranking_checkpoint_path=summary['deep_ranking_best_ckpt'],\n", "    n_investors=summary['n_investors'],\n", "    n_deals=summary['n_deals'],\n", "    feature_dims=summary['feature_dims']\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Single Model Evaluation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Evaluate Two-Tower Model\n", "print(\"Evaluating Two-Tower Model...\")\n", "two_tower_metrics = evaluate_metrics_implicit(\n", "    two_tower_model,\n", "    test_interactions,\n", "    train_interactions,\n", "    investor_df,\n", "    deal_df,\n", "    all_deals=np.arange(len(deal_df)),\n", "    K=10\n", ")\n", "\n", "print(\"\\nTwo-Tower Metrics:\")\n", "for metric, value in two_tower_metrics.items():\n", "    print(f\"{metric}: {value:.4f}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Evaluate Deep Ranking Model\n", "print(\"\\nEvaluating Deep Ranking Model...\")\n", "deep_ranking_metrics = evaluate_metrics_implicit(\n", "    deep_ranking_model,\n", "    test_interactions,\n", "    train_interactions,\n", "    investor_df,\n", "    deal_df,\n", "    all_deals=np.arange(len(deal_df)),\n", "    K=10\n", ")\n", "\n", "print(\"\\nDeep Ranking Metrics:\")\n", "for metric, value in deep_ranking_metrics.items():\n", "    print(f\"{metric}: {value:.4f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Two-Stage Pipeline Evaluation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Evaluate Two-Stage System\n", "print(\"\\nEvaluating Two-Stage Pipeline...\")\n", "two_stage_metrics = evaluate_two_stage_pipeline(\n", "    retrieval_model=two_tower_model,\n", "    ranking_model=deep_ranking_model,\n", "    test_interactions=test_interactions,\n", "    train_interactions=train_interactions,\n", "    investor_df=investor_df,\n", "    deal_df=deal_df,\n", "    all_deals=np.arange(len(deal_df)),\n", "    retrieval_k=100,\n", "    final_k=10\n", ")\n", "\n", "print(\"\\nTwo-Stage Pipeline Metrics:\")\n", "for metric, value in two_stage_metrics.items():\n", "    print(f\"{metric}: {value:.4f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Detailed Analysis"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compare metrics\n", "metrics_names = list(two_tower_metrics.keys())\n", "x = np.arange(len(metrics_names))\n", "width = 0.25\n", "\n", "fig, ax = plt.subplots(figsize=(12, 6))\n", "ax.bar(x - width, [two_tower_metrics[m] for m in metrics_names], \n", "       width, label='Two-Tower', alpha=0.8)\n", "ax.bar(x, [deep_ranking_metrics[m] for m in metrics_names], \n", "       width, label='Deep Ranking', alpha=0.8)\n", "ax.bar(x + width, [two_stage_metrics[m] for m in metrics_names], \n", "       width, label='Two-Stage', alpha=0.8)\n", "\n", "ax.set_xlabel('Metrics')\n", "ax.set_ylabel('Score')\n", "ax.set_title('Model Performance Comparison')\n", "ax.set_xticks(x)\n", "ax.set_xticklabels(metrics_names)\n", "ax.legend()\n", "\n", "plt.tight_layout()\n", "plt.savefig('models/performance_comparison.png')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Score Distribution Analysis"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Analyze score distributions for sample investors\n", "sample_investors = test_interactions['investorId'].unique()[:5]\n", "\n", "plot_score_distributions(\n", "    two_tower_model,\n", "    deep_ranking_model,\n", "    sample_investors,\n", "    investor_df,\n", "    deal_df,\n", "    all_deals=np.arange(len(deal_df))\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Performance by User Segments"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Analyze performance by investor type\n", "results_by_type = {}\n", "\n", "investor_types = ['Equity', 'Debt', 'Infrastructure']\n", "\n", "for inv_type in investor_df['type'].unique():\n", "    type_investors = investor_df[investor_df['type'] == inv_type].index\n", "    type_interactions = test_interactions[\n", "        test_interactions['investorId'].isin(type_investors)\n", "    ]\n", "    \n", "    if len(type_interactions) > 0:\n", "        metrics = evaluate_metrics_implicit(\n", "            two_tower_model,\n", "            type_interactions,\n", "            train_interactions,\n", "            investor_df,\n", "            deal_df,\n", "            all_deals=np.arange(len(deal_df)),\n", "            K=10\n", "        )\n", "        results_by_type[inv_type] = metrics\n", "\n", "print(\"Performance by Investor Type:\")\n", "for inv_type, metrics in results_by_type.items():\n", "    type_name = investor_types[inv_type] if inv_type < len(investor_types) else f\"Type {inv_type}\"\n", "    print(f\"\\n{type_name}:\")\n", "    for metric, value in metrics.items():\n", "        print(f\"  {metric}: {value:.4f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Save Evaluation Results"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["eval_results = {\n", "    'two_tower_metrics': two_tower_metrics,\n", "    'deep_ranking_metrics': deep_ranking_metrics,\n", "    'two_stage_metrics': two_stage_metrics,\n", "    'results_by_type': results_by_type,\n", "    'test_size': len(test_interactions)\n", "}\n", "\n", "with open('models/evaluation_results.json', 'w') as f:\n", "    json.dump(eval_results, f, indent=2)\n", "\n", "print(\"\\nEvaluation complete! Results saved to models/evaluation_results.json\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.9"}}, "nbformat": 4, "nbformat_minor": 4}
